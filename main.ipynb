{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:43:27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm,tqdm_notebook \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "START_TIME = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "print(START_TIME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用户行为特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，我们注意到，每一笔信用卡的交易记录都有交易时间，而对于时间字段和文本字段，普通的批量创建特征的方法都是无法较好的挖掘其全部信息的，因此我们需要围绕交易字段中的交易时间进行额外的特征衍生。此处我们可以考虑构造一些用于描述用户行为习惯的特征（经过反复验证，用户行为特征是最为有效的提高预测结果的特征类），包括最近一次交易与首次交易的时间差、信用卡激活日期与首次交易的时间差、用户两次交易平均时间间隔、按照不同交易地点/商品品类进行聚合（并统计均值、方差等统计量）。      \n",
    "&emsp;&emsp;此外，我们也知道越是接近当前时间点的用户行为越有价值，因此我们还需要重点关注用户最近两个月（实际时间跨度可以自行决定）的行为特征，以两个月为跨度，进一步统计该时间周期内用户的上述交易行为特点，并带入模型进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二阶交叉特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在此前的特征衍生过程中，我们曾进行了交叉特征衍生，但只是进行了一阶交叉衍生，例如交易额在不同商品上的汇总，但实际上还可以进一步构造二阶衍生，例如交易额在不同商品组合上的汇总。通常来说更高阶的衍生会导致特征矩阵变得更加稀疏，并且由于每一阶的衍生都会创造大量特征，因此更高阶的衍生往往也会造成维度爆炸，因此高阶交叉特征衍生需要谨慎。不过正如此前我们考虑的，由于用户行为特征对模型结果有更大的影响，因此我们可以单独围绕用户行为数据进行二阶交叉特征衍生，并在后续建模前进行特征筛选。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 异常值识别特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/neGfhCq5xBcO9uL.png\" alt=\"image-20211210155414409\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而若我们更进一步就此前的建模结果展开分析的话，我们会发现，此前的模型误差大多数都源于对异常值用户(card_id)的预测结果。而根据Day 1的讨论我们知道，实际上用户评分是通过某套公式人工计算得出的，因此这些异常值极有可能是某类特殊用户的标记，因此我们不妨在实际建模过程中进行两阶段建模，即先预测每个输入样本是否是异常样本，并根据分类预测结果进行后续的回归建模，基本流程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/PtW9zf6EpchjK4o.png\" alt=\"image-20211210161251760\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而为了保证后续两阶段建模时第一阶段的分类模型能够更加准确的识别异常用户，我们需要创建一些基于异常用户的特征聚合字段，例如异常用户平均消费次数、消费金额等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.模型优化思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 新增CatBoost模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;相比特征优化思路，模型优化的思路相对简单，首先从模型选择来看，相较LightGBM和XGBoost，随机森林对当前数据集的预测能力较弱，可以考虑将其换成集成算法新秀：CatBoost。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而在后续的建模环节中，我们就将使用CatBoost替换随机森林，并最终带入CatBoost、XGBoost和LightGBM三个模型进行模型融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二阶段建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而从模型训练流程角度出发，则可以考虑进行二阶段建模，基本流程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/ioPh1C5uzVgE9dZ.png\" alt=\"image-20211210164131623\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;并且，需要注意的是，在实际二阶段建模过程时，我们需要在每个建模阶段都进行交叉验证与模型融合，才能最大化提升模型效果。也就是说我们需要训练三组模型（以及对应进行三次模型融合），来完成分类预测问题、普通用户回归预测问题和异常用户回归预测问题。三轮建模关系如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/a19mJpQOkMunDbf.png\" alt=\"image-20211210165529875\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难发现，整体建模与融合过程都将变得更加复杂。不过这也是更加贴近真实状态的一种情况，很多时候算法和融合过程都只是元素，如何构建一个更加精准、高效的训练流程，才是进阶的算法工程人员更需要考虑的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.其他注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 内存管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于接下来我们需要反复读取数据文件并进行计算，因此需要时刻注意进行内存管理，除了可以通过及时删除不用的变量并使用动态垃圾回收机制来清理内存外，还可以使用如下方式在定义数据类型时尽可能在不影响数值运算的前提下给出更加合适的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "new_transactions = pd.read_csv('data/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "historical_transactions = pd.read_csv('data/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "for col in ['authorized_flag', 'category_1']:\n",
    "    historical_transactions[col] = historical_transactions[col].map({'Y':1, 'N':0})\n",
    "    new_transactions[col]        = new_transactions[col].map({'Y':1, 'N':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>二、数据预处理与特征优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此处由于需要进行特征优化，因此数据处理工作需要从头开始执行。首先需要进行数据读取，并进行缺失值处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.36 s, sys: 1.83 s, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 加载训练集，测试集，基本处理\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "target = train['target']\n",
    "for df in [train, test]:    \n",
    "    df['year']  = df['first_active_month'].fillna('0-0').apply(lambda x:int(str(x).split('-')[0]))\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018,3, 1) - df['first_active_month'].dt.date).dt.days\n",
    "    \n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['dayofyear'] = df['first_active_month'].dt.dayofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    \n",
    "## 交易表合并train test\n",
    "train_test = pd.concat([train[['card_id','first_active_month']], test[['card_id','first_active_month']] ], axis=0, ignore_index=True)\n",
    "historical_transactions   = historical_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')\n",
    "new_transactions = new_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后需要进行时间字段的更细粒度的呈现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 3192.83 Mb (64.1% reduction)\n",
      "Mem. usage decreased to 211.55 Mb (64.7% reduction)\n",
      "CPU times: user 5min 52s, sys: 34.7 s, total: 6min 27s\n",
      "Wall time: 6min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def month_trans(x): \n",
    "    return x // 30\n",
    "\n",
    "def week_trans(x): \n",
    "    return x // 7\n",
    "\n",
    "## 交易表预处理\n",
    "def get_expand_common(df_):\n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    df['installments'].replace(999, np.nan,inplace=True)\n",
    "    df['installments'].replace(0, 1,inplace=True)\n",
    "    \n",
    "    df['purchase_amount'] = np.round(df['purchase_amount'] / 0.00150265118 + 497.06,8)\n",
    "    df['purchase_amount'] = df.purchase_amount.apply(lambda x: np.round(x))\n",
    "    \n",
    "    df['purchase_date']          =  pd.to_datetime(df['purchase_date']) \n",
    "    df['first_active_month']     =  pd.to_datetime(df['first_active_month']) \n",
    "    df['purchase_hour']          =  df['purchase_date'].dt.hour\n",
    "    df['year']                   = df['purchase_date'].dt.year\n",
    "    df['month']                  =  df['purchase_date'].dt.month\n",
    "    df['day']                    = df['purchase_date'].dt.day\n",
    "    df['hour']                   = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['dayofweek']              =  df['purchase_date'].dt.dayofweek\n",
    "    df['weekend']                =  (df.purchase_date.dt.weekday >=5).astype(int) \n",
    "    df                           =  df.sort_values(['card_id','purchase_date']) \n",
    "    df['purchase_date_floorday'] =  df['purchase_date'].dt.floor('d')  #删除小于day的时间\n",
    "    \n",
    "    # 距离激活时间的相对时间,0, 1,2,3,...,max-act\n",
    "    df['purchase_day_since_active_day']   = df['purchase_date_floorday'] - df['first_active_month']  #ht_card_id_gp['purchase_date_floorday'].transform('min')\n",
    "    df['purchase_day_since_active_day']   = df['purchase_day_since_active_day'].dt.days  #.astype('timedelta64[D]') \n",
    "    df['purchase_month_since_active_day'] = df['purchase_day_since_active_day'].agg(month_trans).values\n",
    "    df['purchase_week_since_active_day']  = df['purchase_day_since_active_day'].agg(week_trans).values\n",
    "    \n",
    "    # 距离最后一天时间的相对时间,0,1,2,3,...,max-act\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    df['purchase_day_since_reference_day']   =  ht_card_id_gp['purchase_date_floorday'].transform('max') - df['purchase_date_floorday']\n",
    "    df['purchase_day_since_reference_day']   =  df['purchase_day_since_reference_day'].dt.days\n",
    "    # 一个粗粒度的特征(距离最近购买过去了几周，几月)\n",
    "    df['purchase_week_since_reference_day']  = df['purchase_day_since_reference_day'].agg(week_trans).values\n",
    "    df['purchase_month_since_reference_day'] = df['purchase_day_since_reference_day'].agg(month_trans).values\n",
    "    \n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].shift()\n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].values - df['purchase_day_diff'].values\n",
    "    df['purchase_day_diff']   =  df['purchase_day_diff'].dt.days\n",
    "    df['purchase_week_diff']  =  df['purchase_day_diff'].agg(week_trans).values\n",
    "    df['purchase_month_diff'] =  df['purchase_day_diff'].agg(month_trans).values \n",
    "    \n",
    "    df['purchase_amount_ddgd_98']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.98**x).values\n",
    "    df['purchase_amount_ddgd_99']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.99**x).values    \n",
    "    df['purchase_amount_wdgd_96']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.96**x).values \n",
    "    df['purchase_amount_wdgd_97']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.97**x).values \n",
    "    df['purchase_amount_mdgd_90']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.9**x).values\n",
    "    df['purchase_amount_mdgd_80']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.8**x).values \n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "historical_transactions = get_expand_common(historical_transactions)\n",
    "new_transactions        = get_expand_common(new_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.特征优化部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29112361, 39)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_transactions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在执行完数据清洗与时间字段的处理之后，接下来我们需要开始执行特征优化。根据此前介绍的思路，首先我们需要进行基础行为特征字段衍生："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate statistics features...\n",
      "generate statistics features...\n",
      "generate statistics features...\n",
      "CPU times: user 4min 22s, sys: 10.6 s, total: 4min 32s\n",
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 构造基本统计特征\n",
    "def aggregate_transactions(df_, prefix): \n",
    "    \n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "    df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "    \n",
    "    df.loc[:, 'purchase_date'] = pd.DatetimeIndex(df['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1':      ['mean'],\n",
    "        'category_2':      ['mean'],\n",
    "        'category_3':      ['mean'],\n",
    "        'installments':    ['mean', 'max', 'min', 'std'],\n",
    "        'month_lag':       ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'month':           ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'hour':            ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'weekofyear':      ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'dayofweek':       ['nunique', 'mean'],\n",
    "        'weekend':         ['mean'],\n",
    "        'year':            ['nunique'],\n",
    "        'card_id':         ['size','count'],\n",
    "        'purchase_date':   ['max', 'min'],\n",
    "        ###\n",
    "        'price':             ['mean','max','min','std'],\n",
    "        'duration':          ['mean','min','max','std','skew'],\n",
    "        'amount_month_ratio':['mean','min','max','std','skew'],\n",
    "        } \n",
    "    \n",
    "    for col in ['category_2','category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        agg_func[col+'_mean'] = ['mean']\n",
    "    \n",
    "    agg_df = df.groupby(['card_id']).agg(agg_func)\n",
    "    agg_df.columns = [prefix + '_'.join(col).strip() for col in agg_df.columns.values]\n",
    "    agg_df.reset_index(drop=False, inplace=True)\n",
    "  \n",
    "    return agg_df\n",
    "print('generate statistics features...')\n",
    "auth_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==1], prefix='auth_')\n",
    "print('generate statistics features...')\n",
    "hist_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==0], prefix='hist_')\n",
    "print('generate statistics features...')\n",
    "new_base_stat  = aggregate_transactions(new_transactions, prefix='new_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      "reference_day 不存在！！！\n",
      "first_day 不存在！！！\n",
      "last_day 不存在！！！\n",
      "activation_day 不存在！！！\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529126dbaf324a78a73528833dfce7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21551dbcf8c450489ec7125f50f2c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5dfbd7ade1045d498d37d9a0a567a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 112.08 Mb (73.1% reduction)\n",
      "hist...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      "card_id(month_lag, min to reference day):min\n",
      "last_day 不存在！！！\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ff23d5193a4e1f83bb71e70571372d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1856ed9bd47466da2027f9e85d221e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6f2d6aa6684d448a726f5e5aed7ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 121.08 Mb (72.6% reduction)\n",
      "new...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      " Eight time features, \n",
      "card_id(month_lag, min to reference day):min\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd5fe64f5114bce8840694895f6c621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce554f91d72c434a9bd6cda981afab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1850a06f774559bab8090e450cb1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Part2， data with time less than activation day ******************************\n",
      "card_id(purchase_amount): sum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a53db94257485fb14c7a403c295b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 92.65 Mb (73.0% reduction)\n",
      "CPU times: user 9min 10s, sys: 1min 39s, total: 10min 49s\n",
      "Wall time: 10min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_quantile(x, percentiles = [0.1, 0.25, 0.75, 0.9]):\n",
    "    x_len = len(x)\n",
    "    x = np.sort(x)\n",
    "    sts_feas = []  \n",
    "    for per_ in percentiles:\n",
    "        if per_ == 1:\n",
    "            sts_feas.append(x[x_len - 1]) \n",
    "        else:\n",
    "            sts_feas.append(x[int(x_len * per_)]) \n",
    "    return sts_feas \n",
    "\n",
    "def get_cardf_tran(df_, month = 3, prefix = '_'):\n",
    "    \n",
    "    df = df_.copy() \n",
    "    if prefix == 'hist_cardf_':\n",
    "        df['month_to_now']  =  (datetime.date(2018, month, 1) - df['purchase_date_floorday'].dt.date).dt.days\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id') \n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    if  prefix == 'hist_cardf_':\n",
    "        cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values\n",
    "        cardid_features['card_id_isau_sum'] = ht_card_id_gp['authorized_flag'].sum().values \n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "    cardid_features['month_diff_median'] = ht_card_id_gp['month_diff'].median().values\n",
    "    \n",
    "    if prefix == 'hist_cardf_':\n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "       \n",
    "        # first to activation day\n",
    "        cardid_features['first_to_activation_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to reference day \n",
    "        cardid_features['activation_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['activation_day']).dt.days\n",
    "        # first to last day \n",
    "        cardid_features['first_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['first_day']).dt.days\n",
    "        # reference day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_min'] = ht_card_id_gp['month_lag'].agg('min').values   \n",
    "        # is_purchase_before_activation,first_to_reference_day_divide_activation_to_reference_day\n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['first_to_activation_day'] < 0 \n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['is_purchase_before_activation'].astype(int)\n",
    "        cardid_features['first_to_reference_day_divide_activation_to_reference_day'] = cardid_features['first_to_reference_day']  / (cardid_features['activation_to_reference_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_reference_day'].values / cardid_features['card_id_cnt'].values\n",
    "   \n",
    "    if prefix == 'new_cardf_':\n",
    "        print(' Eight time features, ') \n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['reference_day'].last().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['last_day']                =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "        # reference to first day\n",
    "        cardid_features['reference_day_to_first_day']  =  (cardid_features['first_day'] - cardid_features['reference_day']).dt.days\n",
    "        # reference to last day\n",
    "        cardid_features['reference_day_to_last_day']  =  (cardid_features['last_day'] - cardid_features['reference_day']).dt.days  \n",
    "        # first to last day \n",
    "        cardid_features['first_to_last_day']  =  (cardid_features['last_day'] - cardid_features['first_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_first_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_last_day']  =  (cardid_features['last_day'] - cardid_features['activation_day']).dt.days\n",
    "        # last day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_max'] = ht_card_id_gp['month_lag'].agg('max').values  \n",
    "        cardid_features['first_to_last_day_divide_reference_to_last_day'] = cardid_features['first_to_last_day']  / (cardid_features['reference_day_to_last_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_last_day'].values / cardid_features['card_id_cnt'].values\n",
    "    \n",
    "    for f in ['reference_day', 'first_day', 'last_day', 'activation_day']:\n",
    "        try:\n",
    "            del cardid_features[f]\n",
    "        except:\n",
    "            print(f, '不存在！！！')\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['category_1','category_2','category_3','state_id','city_id','installments','merchant_id', 'merchant_category_id','subsector_id','month_lag','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col]            =  ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] =  cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    print('card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)') \n",
    "    for col in tqdm_notebook(['installments','purchase_amount','purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median','max','min']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values\n",
    "            \n",
    "            cardid_features['card_id_' +col+ '_range'] =  cardid_features['card_id_' +col+ '_max'].values - cardid_features['card_id_' +col+ '_min'].values\n",
    "            percentiles = ht_card_id_gp[col].apply(lambda x:get_quantile(x,percentiles = [0.025, 0.25, 0.75, 0.975])) \n",
    "\n",
    "            cardid_features[col + '_2.5_quantile']  = percentiles.map(lambda x:x[0]).values\n",
    "            cardid_features[col + '_25_quantile'] = percentiles.map(lambda x:x[1]).values\n",
    "            cardid_features[col + '_75_quantile'] = percentiles.map(lambda x:x[2]).values\n",
    "            cardid_features[col + '_97.5_quantile'] = percentiles.map(lambda x:x[3]).values\n",
    "            cardid_features['card_id_' +col+ '_range2'] =  cardid_features[col+ '_97.5_quantile'].values - cardid_features[col+ '_2.5_quantile'].values\n",
    "            del cardid_features[col + '_2.5_quantile'],cardid_features[col + '_97.5_quantile']\n",
    "            gc.collect()\n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values          \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大installments\n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id',,\n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "    \n",
    "    if prefix == 'new_cardf_':\n",
    "    ######## 在卡未激活之前就有过消费的记录  ##############   \n",
    "        print('*'*30,'Part2， data with time less than activation day','*'*30)\n",
    "        df_part = df.loc[df.purchase_date < df.first_active_month]\n",
    "\n",
    "        cardid_features_part = pd.DataFrame()\n",
    "        cardid_features_part['card_id'] = df_part['card_id'].unique()   \n",
    "        ht_card_id_part_gp = df_part.groupby('card_id')\n",
    "        cardid_features_part['card_id_part_cnt'] = ht_card_id_part_gp['authorized_flag'].count().values\n",
    "\n",
    "        print('card_id(purchase_amount): sum') \n",
    "        for col in tqdm_notebook(['purchase_amount']): \n",
    "            for opt in ['sum','mean']:\n",
    "                cardid_features_part['card_id_part_' +col+ '_' + opt] = ht_card_id_part_gp[col].agg(opt).values\n",
    "\n",
    "        cardid_features = cardid_features.merge(cardid_features_part, on ='card_id', how='left')\n",
    "        cardid_features['card_id_part_purchase_amount_sum_percent'] = cardid_features['card_id_part_purchase_amount_sum'] / (cardid_features['card_id_purchase_amount_sum'] + 0.01)\n",
    "\n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features\n",
    "print('auth...')\n",
    "authorized_transactions = historical_transactions.loc[historical_transactions['authorized_flag'] == 1]\n",
    "auth_cardf_tran = get_cardf_tran(authorized_transactions, 3, prefix='auth_cardf_')\n",
    "print('hist...')\n",
    "hist_cardf_tran = get_cardf_tran(historical_transactions, 3, prefix='hist_cardf_')\n",
    "print('new...')\n",
    "reference_days = historical_transactions.groupby('card_id')['purchase_date'].last().to_frame('reference_day')\n",
    "reference_days.reset_index(inplace = True)\n",
    "new_transactions = new_transactions.merge(reference_days, on ='card_id', how='left')\n",
    "new_cardf_tran  = get_cardf_tran(new_transactions, 5, prefix='new_cardf_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，需要进一步考虑最近两个月的用户行为特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      " card id : count\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648586dd95004d41aa27ee998aafa677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f22dfb7a704e92bec93c04d86f88c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a9841d1432454b8adc2ec15aeac877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 84.45 Mb (74.0% reduction)\n",
      "CPU times: user 1min 22s, sys: 4.9 s, total: 1min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_cardf_tran_last2(df_, month = 3, prefix = 'last2_'): \n",
    "    \n",
    "    df = df_.loc[df_.month_lag >= -2].copy()\n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    print(' card id : count')\n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values \n",
    "    cardid_features['card_id_isau_sum']  = ht_card_id_gp['authorized_flag'].sum().values\n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['state_id','city_id','installments','merchant_id', 'merchant_category_id','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col] = ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] = cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    for col in tqdm_notebook(['purchase_amount','purchase_amount_ddgd_98','purchase_amount_wdgd_96','purchase_amount_mdgd_90','purchase_amount_mdgd_80']): #,'purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values  \n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大\n",
    "    \n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id', \n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "     \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "hist_cardf_tran_last2 = get_cardf_tran_last2(historical_transactions, month = 3, prefix = 'hist_last2_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及进一步进行二阶交叉特征衍生："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ddd4dc99224600af4bd13ea0309483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcced651734f43dcbd41dbb1ff8c4148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d9674fd88b42afa622071059da288f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 70.16 Mb (66.8% reduction)\n",
      "CPU times: user 5min 24s, sys: 1min 20s, total: 6min 45s\n",
      "Wall time: 6min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def successive_aggregates(df_, prefix = 'levelAB_'):\n",
    "    df = df_.copy()\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()    \n",
    "     \n",
    "    level12_nunique = [('month_lag','state_id'),('month_lag','city_id'),('month_lag','subsector_id'),('month_lag','merchant_category_id'),('month_lag','merchant_id'),('month_lag','purchase_date_floorday'),\\\n",
    "                       ('subsector_id','merchant_category_id'),('subsector_id','merchant_id'),('subsector_id','purchase_date_floorday'),('subsector_id','month_lag'),\\\n",
    "                       ('merchant_category_id', 'merchant_id'),('merchant_category_id','purchase_date_floorday'),('merchant_category_id','month_lag'),\\\n",
    "                       ('purchase_date_floorday', 'merchant_id'),('purchase_date_floorday','merchant_category_id'),('purchase_date_floorday','subsector_id')]    \n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_nunique):  \n",
    "        \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].nunique().to_frame(col_level2 + '_nunique')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_nunique'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_nunique_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_count = ['month_lag','state_id','city_id','subsector_id','merchant_category_id','merchant_id','purchase_date_floorday']\n",
    "    for col_level in tqdm_notebook(level12_count): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level])['merchant_id'].count().to_frame(col_level + '_count')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level + '_count'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level + '_count_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_meansum = [('month_lag','purchase_amount'),('state_id','purchase_amount'),('city_id','purchase_amount'),('subsector_id','purchase_amount'),\\\n",
    "                       ('merchant_category_id','purchase_amount'),('merchant_id','purchase_amount'),('purchase_date_floorday','purchase_amount')]\n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_meansum): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].sum().to_frame(col_level2 + '_sum')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_sum'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_sum_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "\n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left')           \n",
    "    \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "print('hist...')\n",
    "hist_levelAB = successive_aggregates(historical_transactions, prefix = 'hist_levelAB_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，将上述衍生特征进行合并："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 11)\n",
      "(123623, 10)\n",
      "#_____基础统计特征\n",
      "(201917, 164)\n",
      "(123623, 163)\n",
      "#_____全局cardid特征\n",
      "(201917, 687)\n",
      "(123623, 686)\n",
      "#_____最近两月cardid特征\n",
      "(201917, 821)\n",
      "(123623, 820)\n",
      "#_____补充二阶特征\n",
      "(201917, 911)\n",
      "(123623, 910)\n",
      "CPU times: user 8.81 s, sys: 904 ms, total: 9.71 s\n",
      "Wall time: 9.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "## 合并到训练集和测试集\n",
    "print('#_____基础统计特征')\n",
    "train = pd.merge(train, auth_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, new_base_stat , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_base_stat , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____全局cardid特征')\n",
    "train = pd.merge(train, auth_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, new_cardf_tran , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_cardf_tran , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____最近两月cardid特征')\n",
    "train = pd.merge(train, hist_cardf_tran_last2, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran_last2, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____补充二阶特征')\n",
    "train = pd.merge(train, hist_levelAB, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_levelAB, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并在此基础上补充部分简单四折运算后的衍生特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 569 ms, sys: 296 ms, total: 865 ms\n",
      "Wall time: 864 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    colname = f+'_outliers_mean'\n",
    "    order_label = train.groupby([f])['outliers'].mean()\n",
    "    for df in [train, test]:\n",
    "        df[colname] = df[f].map(order_label)\n",
    "\n",
    "for df in [train, test]:\n",
    "    \n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "    \n",
    "    df['card_id_total'] = df['hist_card_id_size']+df['new_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['hist_card_id_count']+df['new_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['hist_cardf_card_id_purchase_amount_sum']+df['new_cardf_card_id_purchase_amount_sum']\n",
    "    df['purchase_amount_ratio'] = df['new_cardf_card_id_purchase_amount_sum']/df['hist_cardf_card_id_purchase_amount_sum']\n",
    "    df['month_diff_ratio'] = df['new_cardf_month_diff_mean']/df['hist_cardf_month_diff_mean']\n",
    "    df['installments_total'] = df['new_cardf_card_id_installments_sum']+df['auth_cardf_card_id_installments_sum']\n",
    "    df['installments_ratio'] = df['new_cardf_card_id_installments_sum']/df['auth_cardf_card_id_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total']/df['installments_total']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_cardf_card_id_purchase_amount_sum'] / df['new_cardf_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_cardf_card_id_purchase_amount_sum'] / df['hist_cardf_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.特征筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在创建完全部特征后即可进行特征筛选了。此处我们考虑手动进行特征筛选，排除部分过于稀疏的特征后即可将数据保存在本地："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除前: 938\n",
      "删除后: 770\n",
      "CPU times: user 1min 53s, sys: 2.78 s, total: 1min 56s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "del_cols = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'new_cardf': \n",
    "        del_cols.append(col)\n",
    "del_cols1 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'hist_last2_' in col:\n",
    "        del_cols1.append(col)\n",
    "del_cols2 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'auth_cardf' in col:\n",
    "        del_cols2.append(col)\n",
    "del_cols3 = []\n",
    "for col in train.columns:\n",
    "    if 'merchant_category_id_month_lag_nunique_' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'city_id' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff' in col and 'hist_last2_' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff_std' in col or 'month_diff_gap' in col:\n",
    "        del_cols3.append(col) \n",
    "fea_cols = [col for col in train.columns if train[col].dtypes!='object' and train[col].dtypes != '<M8[ns]' and col!='target' not in col and col!='min_num'\\\n",
    "            and col not in del_cols and col not in del_cols1 and col not in del_cols2 and col!='target1' and col!='card_id_cnt_ht_pivot_supp'  and col not in del_cols3]   \n",
    "print('删除前:',train.shape[1])\n",
    "print('删除后:',len(fea_cols))\n",
    "\n",
    "train = train[fea_cols+['target']]\n",
    "fea_cols.remove('outliers')\n",
    "test = test[fea_cols]\n",
    "\n",
    "train.to_csv('./data/all_train_features.csv',index=False)\n",
    "test.to_csv('./data/all_test_features.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际执行过程中，可以按照如下方式进行读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (201917, 771)\n",
      "ntrain: (199710, 771)\n",
      "CPU times: user 25.5 s, sys: 14.7 s, total: 40.2 s\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load all features\n",
    "train = pd.read_csv('./data/all_train_features.csv')\n",
    "test  = pd.read_csv('./data/all_test_features.csv')\n",
    "\n",
    "# 异常数据集\n",
    "ntrain = train[train['outliers'] == 0]\n",
    "inf_cols = ['new_cardf_card_id_cnt_divide_installments_nunique', 'hist_last2_card_id_cnt_divide_installments_nunique']\n",
    "train[inf_cols] = train[inf_cols].replace(np.inf, train[inf_cols].replace(np.inf, -99).max().max())\n",
    "ntrain[inf_cols] = ntrain[inf_cols].replace(np.inf, ntrain[inf_cols].replace(np.inf, -99).max().max())\n",
    "test[inf_cols] = test[inf_cols].replace(np.inf, test[inf_cols].replace(np.inf, -99).max().max())\n",
    "\n",
    "# ## load sparse\n",
    "# train_tags = sparse.load_npz('train_tags.npz')\n",
    "# test_tags  = sparse.load_npz('test_tags.npz')\n",
    "\n",
    "## 获取非异常值的index\n",
    "normal_index = train[train['outliers']==0].index.tolist()\n",
    "## without outliers\n",
    "# ntrain = train[train['outliers'] == 0]\n",
    "\n",
    "target        = train['target'].values\n",
    "ntarget       = ntrain['target'].values\n",
    "target_binary = train['outliers'].values\n",
    "###\n",
    "y_train        = target\n",
    "y_ntrain       = ntarget\n",
    "y_train_binary = target_binary\n",
    "\n",
    "print('train:',train.shape)\n",
    "print('ntrain:',ntrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>二、两阶段建模优化\n",
    "## <center> LightGBM+XGBoost+CatBoost两阶段建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/a19mJpQOkMunDbf.png\" alt=\"image-20211210165529875\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;为了方便更快速的调用三种不同的模型，并且同时要求能够完成分类和回归预测，此处通过定义一个函数来完成所有模型的训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds, model_type='lgb', eval_type='regression'):\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            trn_data = lgb.Dataset(X[trn_idx], y[trn_idx])\n",
    "            val_data = lgb.Dataset(X[val_idx], y[val_idx])\n",
    "            clf = lgb.train(params, trn_data, num_boost_round=20000, \n",
    "                            valid_sets=[trn_data, val_data], \n",
    "                            verbose_eval=100, early_stopping_rounds=300)\n",
    "            oof[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n",
    "            predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        \n",
    "        if model_type == 'xgb':\n",
    "            trn_data = xgb.DMatrix(X[trn_idx], y[trn_idx])\n",
    "            val_data = xgb.DMatrix(X[val_idx], y[val_idx])\n",
    "            watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "            clf = xgb.train(dtrain=trn_data, num_boost_round=20000, \n",
    "                            evals=watchlist, early_stopping_rounds=200, \n",
    "                            verbose_eval=100, params=params)\n",
    "            oof[val_idx] = clf.predict(xgb.DMatrix(X[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "            predictions += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "        \n",
    "        if (model_type == 'cat') and (eval_type == 'regression'):\n",
    "            clf = CatBoostRegressor(iterations=20000, eval_metric='RMSE', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict(X[val_idx])\n",
    "            predictions += clf.predict(X_test) / folds.n_splits\n",
    "            \n",
    "        if (model_type == 'cat') and (eval_type == 'binary'):\n",
    "            clf = CatBoostClassifier(iterations=20000, eval_metric='Logloss', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict_proba(X[val_idx])[:,1]\n",
    "            predictions += clf.predict_proba(X_test)[:,1] / folds.n_splits\n",
    "        print(predictions)\n",
    "        \n",
    "        if eval_type == 'regression':\n",
    "            scores.append(mean_squared_error(oof[val_idx], y[val_idx])**0.5)\n",
    "        if eval_type == 'binary':\n",
    "            scores.append(log_loss(y[val_idx], oof[val_idx]))\n",
    "        \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return oof, predictions, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LightGBM模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来即可进行LGB模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Tue Nov  8 02:07:26 2022\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.54005\tvalid_1's rmse: 3.78821\n",
      "[200]\ttraining's rmse: 3.39649\tvalid_1's rmse: 3.74999\n",
      "[300]\ttraining's rmse: 3.30417\tvalid_1's rmse: 3.73676\n",
      "[400]\ttraining's rmse: 3.23562\tvalid_1's rmse: 3.73043\n",
      "[500]\ttraining's rmse: 3.1804\tvalid_1's rmse: 3.7276\n",
      "[600]\ttraining's rmse: 3.13195\tvalid_1's rmse: 3.72638\n",
      "[700]\ttraining's rmse: 3.08934\tvalid_1's rmse: 3.72596\n",
      "[800]\ttraining's rmse: 3.0487\tvalid_1's rmse: 3.72557\n",
      "[900]\ttraining's rmse: 3.01162\tvalid_1's rmse: 3.72533\n",
      "[1000]\ttraining's rmse: 2.97761\tvalid_1's rmse: 3.72515\n",
      "[1100]\ttraining's rmse: 2.94447\tvalid_1's rmse: 3.72503\n",
      "[1200]\ttraining's rmse: 2.91096\tvalid_1's rmse: 3.7257\n",
      "[1300]\ttraining's rmse: 2.88135\tvalid_1's rmse: 3.7261\n",
      "Early stopping, best iteration is:\n",
      "[1037]\ttraining's rmse: 2.96521\tvalid_1's rmse: 3.72496\n",
      "[-0.37592769 -0.09387435 -0.18519499 ...  0.14576043 -0.69626056\n",
      "  0.01783658]\n",
      "Fold 1 started at Tue Nov  8 02:09:34 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.57191\tvalid_1's rmse: 3.66059\n",
      "[200]\ttraining's rmse: 3.42454\tvalid_1's rmse: 3.62325\n",
      "[300]\ttraining's rmse: 3.33258\tvalid_1's rmse: 3.61115\n",
      "[400]\ttraining's rmse: 3.26419\tvalid_1's rmse: 3.60453\n",
      "[500]\ttraining's rmse: 3.20836\tvalid_1's rmse: 3.60121\n",
      "[600]\ttraining's rmse: 3.16086\tvalid_1's rmse: 3.59903\n",
      "[700]\ttraining's rmse: 3.11738\tvalid_1's rmse: 3.59807\n",
      "[800]\ttraining's rmse: 3.07675\tvalid_1's rmse: 3.59792\n",
      "[900]\ttraining's rmse: 3.03884\tvalid_1's rmse: 3.59825\n",
      "[1000]\ttraining's rmse: 3.00497\tvalid_1's rmse: 3.5987\n",
      "Early stopping, best iteration is:\n",
      "[792]\ttraining's rmse: 3.07966\tvalid_1's rmse: 3.59764\n",
      "[-0.82209143 -0.18284381 -0.33938854 ...  0.30599466 -1.46851467\n",
      "  0.02175936]\n",
      "Fold 2 started at Tue Nov  8 02:11:17 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.56358\tvalid_1's rmse: 3.69595\n",
      "[200]\ttraining's rmse: 3.41641\tvalid_1's rmse: 3.66146\n",
      "[300]\ttraining's rmse: 3.32329\tvalid_1's rmse: 3.65051\n",
      "[400]\ttraining's rmse: 3.25553\tvalid_1's rmse: 3.64532\n",
      "[500]\ttraining's rmse: 3.20081\tvalid_1's rmse: 3.64267\n",
      "[600]\ttraining's rmse: 3.15259\tvalid_1's rmse: 3.64176\n",
      "[700]\ttraining's rmse: 3.10978\tvalid_1's rmse: 3.64114\n",
      "[800]\ttraining's rmse: 3.07113\tvalid_1's rmse: 3.64068\n",
      "[900]\ttraining's rmse: 3.03507\tvalid_1's rmse: 3.63997\n",
      "[1000]\ttraining's rmse: 3.0014\tvalid_1's rmse: 3.64009\n",
      "[1100]\ttraining's rmse: 2.96951\tvalid_1's rmse: 3.6402\n",
      "Early stopping, best iteration is:\n",
      "[873]\ttraining's rmse: 3.04483\tvalid_1's rmse: 3.6397\n",
      "[-1.16736708 -0.27002538 -0.4975698  ...  0.46334803 -2.01500758\n",
      "  0.03760563]\n",
      "Fold 3 started at Tue Nov  8 02:13:03 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.58335\tvalid_1's rmse: 3.59456\n",
      "[200]\ttraining's rmse: 3.43825\tvalid_1's rmse: 3.56355\n",
      "[300]\ttraining's rmse: 3.34719\tvalid_1's rmse: 3.55599\n",
      "[400]\ttraining's rmse: 3.27943\tvalid_1's rmse: 3.55251\n",
      "[500]\ttraining's rmse: 3.22369\tvalid_1's rmse: 3.551\n",
      "[600]\ttraining's rmse: 3.17419\tvalid_1's rmse: 3.55134\n",
      "[700]\ttraining's rmse: 3.13039\tvalid_1's rmse: 3.55116\n",
      "[800]\ttraining's rmse: 3.09178\tvalid_1's rmse: 3.55165\n",
      "[900]\ttraining's rmse: 3.05472\tvalid_1's rmse: 3.55231\n",
      "Early stopping, best iteration is:\n",
      "[674]\ttraining's rmse: 3.14162\tvalid_1's rmse: 3.55091\n",
      "[-1.46663272 -0.31998297 -0.61983437 ...  0.62864779 -2.76737894\n",
      "  0.05012392]\n",
      "Fold 4 started at Tue Nov  8 02:14:44 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 3.54717\tvalid_1's rmse: 3.75965\n",
      "[200]\ttraining's rmse: 3.40234\tvalid_1's rmse: 3.72239\n",
      "[300]\ttraining's rmse: 3.31047\tvalid_1's rmse: 3.71012\n",
      "[400]\ttraining's rmse: 3.24309\tvalid_1's rmse: 3.70542\n",
      "[500]\ttraining's rmse: 3.18899\tvalid_1's rmse: 3.70304\n",
      "[600]\ttraining's rmse: 3.14183\tvalid_1's rmse: 3.70213\n",
      "[700]\ttraining's rmse: 3.09785\tvalid_1's rmse: 3.70113\n",
      "[800]\ttraining's rmse: 3.05993\tvalid_1's rmse: 3.70057\n",
      "[900]\ttraining's rmse: 3.02298\tvalid_1's rmse: 3.70013\n",
      "[1000]\ttraining's rmse: 2.98796\tvalid_1's rmse: 3.70075\n",
      "[1100]\ttraining's rmse: 2.95577\tvalid_1's rmse: 3.70093\n",
      "Early stopping, best iteration is:\n",
      "[883]\ttraining's rmse: 3.02888\tvalid_1's rmse: 3.69999\n",
      "[-1.85340501 -0.41758408 -0.7980595  ...  0.78523829 -3.49251547\n",
      "  0.06080266]\n",
      "CV mean score: 3.6426, std: 0.0641.\n"
     ]
    }
   ],
   "source": [
    "### lgb\n",
    "lgb_params = {'num_leaves': 63,\n",
    "             'min_data_in_leaf': 32, \n",
    "             'objective':'regression',\n",
    "             'max_depth': -1,\n",
    "             'learning_rate': 0.01,\n",
    "             \"min_child_samples\": 20,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             \"lambda_l1\": 0.1,\n",
    "             \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=4096)\n",
    "X_ntrain = ntrain[fea_cols].values\n",
    "X_train  = train[fea_cols].values\n",
    "X_test   = test[fea_cols].values\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_lgb , predictions_lgb , scores_lgb  = train_model(X_train , X_test, y_train, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Tue Nov  8 02:25:25 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58508\tvalid_1's rmse: 1.57342\n",
      "[200]\ttraining's rmse: 1.54442\tvalid_1's rmse: 1.54786\n",
      "[300]\ttraining's rmse: 1.52297\tvalid_1's rmse: 1.5399\n",
      "[400]\ttraining's rmse: 1.50737\tvalid_1's rmse: 1.53643\n",
      "[500]\ttraining's rmse: 1.49394\tvalid_1's rmse: 1.53467\n",
      "[600]\ttraining's rmse: 1.48188\tvalid_1's rmse: 1.53386\n",
      "[700]\ttraining's rmse: 1.47107\tvalid_1's rmse: 1.53326\n",
      "[800]\ttraining's rmse: 1.46101\tvalid_1's rmse: 1.53307\n",
      "[900]\ttraining's rmse: 1.45138\tvalid_1's rmse: 1.53284\n",
      "[1000]\ttraining's rmse: 1.44219\tvalid_1's rmse: 1.53258\n",
      "[1100]\ttraining's rmse: 1.43324\tvalid_1's rmse: 1.53241\n",
      "[1200]\ttraining's rmse: 1.42453\tvalid_1's rmse: 1.53226\n",
      "[1300]\ttraining's rmse: 1.41607\tvalid_1's rmse: 1.53219\n",
      "[1400]\ttraining's rmse: 1.40753\tvalid_1's rmse: 1.53203\n",
      "[1500]\ttraining's rmse: 1.39914\tvalid_1's rmse: 1.53199\n",
      "[1600]\ttraining's rmse: 1.39106\tvalid_1's rmse: 1.532\n",
      "[1700]\ttraining's rmse: 1.38305\tvalid_1's rmse: 1.53204\n",
      "Early stopping, best iteration is:\n",
      "[1438]\ttraining's rmse: 1.40436\tvalid_1's rmse: 1.53198\n",
      "[-0.05534195 -0.07805947 -0.10146567 ...  0.20934522 -0.14803371\n",
      "  0.02191961]\n",
      "Fold 1 started at Tue Nov  8 02:28:15 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.57464\tvalid_1's rmse: 1.62094\n",
      "[200]\ttraining's rmse: 1.53475\tvalid_1's rmse: 1.59167\n",
      "[300]\ttraining's rmse: 1.5136\tvalid_1's rmse: 1.58229\n",
      "[400]\ttraining's rmse: 1.49802\tvalid_1's rmse: 1.5783\n",
      "[500]\ttraining's rmse: 1.48472\tvalid_1's rmse: 1.57627\n",
      "[600]\ttraining's rmse: 1.47294\tvalid_1's rmse: 1.57522\n",
      "[700]\ttraining's rmse: 1.46208\tvalid_1's rmse: 1.5747\n",
      "[800]\ttraining's rmse: 1.45223\tvalid_1's rmse: 1.57424\n",
      "[900]\ttraining's rmse: 1.44269\tvalid_1's rmse: 1.57407\n",
      "[1000]\ttraining's rmse: 1.43357\tvalid_1's rmse: 1.57394\n",
      "[1100]\ttraining's rmse: 1.42473\tvalid_1's rmse: 1.5739\n",
      "[1200]\ttraining's rmse: 1.41609\tvalid_1's rmse: 1.57384\n",
      "[1300]\ttraining's rmse: 1.40778\tvalid_1's rmse: 1.57391\n",
      "[1400]\ttraining's rmse: 1.39958\tvalid_1's rmse: 1.57401\n",
      "Early stopping, best iteration is:\n",
      "[1191]\ttraining's rmse: 1.41687\tvalid_1's rmse: 1.57383\n",
      "[-0.10891116 -0.12070931 -0.17712083 ...  0.39048112 -0.29323693\n",
      "  0.03489626]\n",
      "Fold 2 started at Tue Nov  8 02:30:40 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58246\tvalid_1's rmse: 1.58699\n",
      "[200]\ttraining's rmse: 1.54257\tvalid_1's rmse: 1.55981\n",
      "[300]\ttraining's rmse: 1.5215\tvalid_1's rmse: 1.5512\n",
      "[400]\ttraining's rmse: 1.50598\tvalid_1's rmse: 1.54737\n",
      "[500]\ttraining's rmse: 1.49272\tvalid_1's rmse: 1.5455\n",
      "[600]\ttraining's rmse: 1.48088\tvalid_1's rmse: 1.54439\n",
      "[700]\ttraining's rmse: 1.47027\tvalid_1's rmse: 1.54377\n",
      "[800]\ttraining's rmse: 1.46029\tvalid_1's rmse: 1.54337\n",
      "[900]\ttraining's rmse: 1.45091\tvalid_1's rmse: 1.54324\n",
      "[1000]\ttraining's rmse: 1.44162\tvalid_1's rmse: 1.54305\n",
      "[1100]\ttraining's rmse: 1.43273\tvalid_1's rmse: 1.54304\n",
      "[1200]\ttraining's rmse: 1.4239\tvalid_1's rmse: 1.54292\n",
      "[1300]\ttraining's rmse: 1.41567\tvalid_1's rmse: 1.54289\n",
      "[1400]\ttraining's rmse: 1.40737\tvalid_1's rmse: 1.54294\n",
      "[1500]\ttraining's rmse: 1.39924\tvalid_1's rmse: 1.5429\n",
      "Early stopping, best iteration is:\n",
      "[1203]\ttraining's rmse: 1.42367\tvalid_1's rmse: 1.54289\n",
      "[-0.16679188 -0.16781086 -0.28268078 ...  0.55509906 -0.42788662\n",
      "  0.06662768]\n",
      "Fold 3 started at Tue Nov  8 02:33:16 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.57978\tvalid_1's rmse: 1.59431\n",
      "[200]\ttraining's rmse: 1.53949\tvalid_1's rmse: 1.56843\n",
      "[300]\ttraining's rmse: 1.51814\tvalid_1's rmse: 1.56071\n",
      "[400]\ttraining's rmse: 1.50245\tvalid_1's rmse: 1.55723\n",
      "[500]\ttraining's rmse: 1.48907\tvalid_1's rmse: 1.5555\n",
      "[600]\ttraining's rmse: 1.47712\tvalid_1's rmse: 1.55474\n",
      "[700]\ttraining's rmse: 1.46616\tvalid_1's rmse: 1.55433\n",
      "[800]\ttraining's rmse: 1.45608\tvalid_1's rmse: 1.5541\n",
      "[900]\ttraining's rmse: 1.44635\tvalid_1's rmse: 1.55386\n",
      "[1000]\ttraining's rmse: 1.43715\tvalid_1's rmse: 1.55371\n",
      "[1100]\ttraining's rmse: 1.4283\tvalid_1's rmse: 1.55365\n",
      "[1200]\ttraining's rmse: 1.41961\tvalid_1's rmse: 1.55374\n",
      "[1300]\ttraining's rmse: 1.41135\tvalid_1's rmse: 1.55382\n",
      "Early stopping, best iteration is:\n",
      "[1097]\ttraining's rmse: 1.42858\tvalid_1's rmse: 1.55364\n",
      "[-0.21563612 -0.23897806 -0.36265669 ...  0.71767985 -0.56419674\n",
      "  0.09642397]\n",
      "Fold 4 started at Tue Nov  8 02:35:40 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's rmse: 1.58192\tvalid_1's rmse: 1.58906\n",
      "[200]\ttraining's rmse: 1.54166\tvalid_1's rmse: 1.56121\n",
      "[300]\ttraining's rmse: 1.52047\tvalid_1's rmse: 1.55234\n",
      "[400]\ttraining's rmse: 1.50498\tvalid_1's rmse: 1.54843\n",
      "[500]\ttraining's rmse: 1.49163\tvalid_1's rmse: 1.54607\n",
      "[600]\ttraining's rmse: 1.47974\tvalid_1's rmse: 1.54496\n",
      "[700]\ttraining's rmse: 1.46885\tvalid_1's rmse: 1.54424\n",
      "[800]\ttraining's rmse: 1.45863\tvalid_1's rmse: 1.54372\n",
      "[900]\ttraining's rmse: 1.44917\tvalid_1's rmse: 1.54348\n",
      "[1000]\ttraining's rmse: 1.43994\tvalid_1's rmse: 1.54329\n",
      "[1100]\ttraining's rmse: 1.43107\tvalid_1's rmse: 1.54307\n",
      "[1200]\ttraining's rmse: 1.42238\tvalid_1's rmse: 1.54291\n",
      "[1300]\ttraining's rmse: 1.41416\tvalid_1's rmse: 1.54286\n",
      "[1400]\ttraining's rmse: 1.40573\tvalid_1's rmse: 1.5429\n",
      "[1500]\ttraining's rmse: 1.39751\tvalid_1's rmse: 1.54274\n",
      "[1600]\ttraining's rmse: 1.38936\tvalid_1's rmse: 1.54281\n",
      "[1700]\ttraining's rmse: 1.38165\tvalid_1's rmse: 1.54275\n",
      "[1800]\ttraining's rmse: 1.37392\tvalid_1's rmse: 1.54285\n",
      "Early stopping, best iteration is:\n",
      "[1505]\ttraining's rmse: 1.39707\tvalid_1's rmse: 1.54271\n",
      "[-0.27579351 -0.2849864  -0.4251475  ...  0.89462537 -0.69102758\n",
      "  0.12637034]\n",
      "CV mean score: 1.5490, std: 0.0142.\n"
     ]
    }
   ],
   "source": [
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nlgb, predictions_nlgb, scores_nlgb = train_model(X_ntrain, X_test, y_ntrain, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 分类模型 ==========\n",
      "Fold 0 started at Tue Nov  8 02:38:33 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0342137\tvalid_1's binary_logloss: 0.0492863\n",
      "[200]\ttraining's binary_logloss: 0.0268448\tvalid_1's binary_logloss: 0.0473058\n",
      "[300]\ttraining's binary_logloss: 0.022271\tvalid_1's binary_logloss: 0.0468433\n",
      "[400]\ttraining's binary_logloss: 0.0191561\tvalid_1's binary_logloss: 0.0467856\n",
      "[500]\ttraining's binary_logloss: 0.0167201\tvalid_1's binary_logloss: 0.046996\n",
      "[600]\ttraining's binary_logloss: 0.0146967\tvalid_1's binary_logloss: 0.0472321\n",
      "Early stopping, best iteration is:\n",
      "[394]\ttraining's binary_logloss: 0.0193216\tvalid_1's binary_logloss: 0.0467753\n",
      "[0.00673749 0.00023672 0.00140346 ... 0.00114199 0.0074994  0.00048727]\n",
      "Fold 1 started at Tue Nov  8 02:40:02 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0350358\tvalid_1's binary_logloss: 0.0458227\n",
      "[200]\ttraining's binary_logloss: 0.0276268\tvalid_1's binary_logloss: 0.0438373\n",
      "[300]\ttraining's binary_logloss: 0.0229776\tvalid_1's binary_logloss: 0.0432551\n",
      "[400]\ttraining's binary_logloss: 0.0198175\tvalid_1's binary_logloss: 0.0431557\n",
      "[500]\ttraining's binary_logloss: 0.0173007\tvalid_1's binary_logloss: 0.0432234\n",
      "[600]\ttraining's binary_logloss: 0.0152226\tvalid_1's binary_logloss: 0.0434176\n",
      "Early stopping, best iteration is:\n",
      "[392]\ttraining's binary_logloss: 0.0200393\tvalid_1's binary_logloss: 0.0431485\n",
      "[0.01383792 0.00063822 0.00320301 ... 0.00268013 0.02088071 0.00104776]\n",
      "Fold 2 started at Tue Nov  8 02:41:25 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0349369\tvalid_1's binary_logloss: 0.0468966\n",
      "[200]\ttraining's binary_logloss: 0.0274141\tvalid_1's binary_logloss: 0.0449266\n",
      "[300]\ttraining's binary_logloss: 0.022792\tvalid_1's binary_logloss: 0.0443669\n",
      "[400]\ttraining's binary_logloss: 0.0196256\tvalid_1's binary_logloss: 0.044293\n",
      "[500]\ttraining's binary_logloss: 0.0171227\tvalid_1's binary_logloss: 0.044319\n",
      "[600]\ttraining's binary_logloss: 0.0150642\tvalid_1's binary_logloss: 0.0445181\n",
      "Early stopping, best iteration is:\n",
      "[381]\ttraining's binary_logloss: 0.0201591\tvalid_1's binary_logloss: 0.0442725\n",
      "[0.02073515 0.0009393  0.00478875 ... 0.00385602 0.03062231 0.00161475]\n",
      "Fold 3 started at Tue Nov  8 02:42:48 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0355798\tvalid_1's binary_logloss: 0.0439932\n",
      "[200]\ttraining's binary_logloss: 0.0279841\tvalid_1's binary_logloss: 0.0421348\n",
      "[300]\ttraining's binary_logloss: 0.0232614\tvalid_1's binary_logloss: 0.041677\n",
      "[400]\ttraining's binary_logloss: 0.0200309\tvalid_1's binary_logloss: 0.0415573\n",
      "[500]\ttraining's binary_logloss: 0.0175207\tvalid_1's binary_logloss: 0.0416581\n",
      "[600]\ttraining's binary_logloss: 0.0154436\tvalid_1's binary_logloss: 0.0418204\n",
      "[700]\ttraining's binary_logloss: 0.0136535\tvalid_1's binary_logloss: 0.0420543\n",
      "Early stopping, best iteration is:\n",
      "[405]\ttraining's binary_logloss: 0.0198935\tvalid_1's binary_logloss: 0.0415458\n",
      "[0.02735361 0.00123548 0.00581347 ... 0.00523717 0.04333759 0.002017  ]\n",
      "Fold 4 started at Tue Nov  8 02:44:18 2022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\ttraining's binary_logloss: 0.0344632\tvalid_1's binary_logloss: 0.0482949\n",
      "[200]\ttraining's binary_logloss: 0.0270882\tvalid_1's binary_logloss: 0.0463436\n",
      "[300]\ttraining's binary_logloss: 0.0225691\tvalid_1's binary_logloss: 0.0457807\n",
      "[400]\ttraining's binary_logloss: 0.0194372\tvalid_1's binary_logloss: 0.045723\n",
      "[500]\ttraining's binary_logloss: 0.0169722\tvalid_1's binary_logloss: 0.0458608\n",
      "[600]\ttraining's binary_logloss: 0.0149096\tvalid_1's binary_logloss: 0.0461018\n",
      "Early stopping, best iteration is:\n",
      "[342]\ttraining's binary_logloss: 0.02115\tvalid_1's binary_logloss: 0.0457078\n",
      "[0.03443624 0.00163074 0.00788527 ... 0.00672145 0.06315408 0.0025155 ]\n",
      "CV mean score: 0.0443, std: 0.0018.\n"
     ]
    }
   ],
   "source": [
    "print('='*10,'分类模型','='*10)\n",
    "lgb_params['objective'] = 'binary'\n",
    "lgb_params['metric']    = 'binary_logloss'\n",
    "oof_blgb, predictions_blgb, scores_blgb = train_model(X_train , X_test, y_train_binary, params=lgb_params, folds=folds, model_type='lgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将所有预测结果进行保存，包括一个分类模型、以及两个回归模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_lgb\n",
    "sub_df.to_csv('predictions_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb  = pd.DataFrame(oof_lgb)\n",
    "oof_nlgb = pd.DataFrame(oof_nlgb)\n",
    "oof_blgb = pd.DataFrame(oof_blgb)\n",
    "\n",
    "predictions_lgb  = pd.DataFrame(predictions_lgb)\n",
    "predictions_nlgb = pd.DataFrame(predictions_nlgb)\n",
    "predictions_blgb = pd.DataFrame(predictions_blgb)\n",
    "\n",
    "oof_lgb.to_csv('./result/oof_lgb.csv',header=None,index=False)\n",
    "oof_blgb.to_csv('./result/oof_blgb.csv',header=None,index=False)\n",
    "oof_nlgb.to_csv('./result/oof_nlgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_lgb.to_csv('./result/predictions_lgb.csv',header=None,index=False)\n",
    "predictions_nlgb.to_csv('./result/predictions_nlgb.csv',header=None,index=False)\n",
    "predictions_blgb.to_csv('./result/predictions_blgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和上一节类似，接下来我们考虑进行结果提交，查看经过特征优化后单模型的建模效果能否得到改善："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/3ePRrd78vUzCuVk.png\" alt=\"image-20211210185918996\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgboost模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进一步进行XGB模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Tue Nov  8 03:19:26 2022\n",
      "[03:19:28] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:19:28] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:3.91358\tvalid_data-rmse:3.93381\n",
      "[100]\ttrain-rmse:3.10484\tvalid_data-rmse:3.67459\n",
      "[200]\ttrain-rmse:2.93277\tvalid_data-rmse:3.68059\n",
      "[289]\ttrain-rmse:2.78722\tvalid_data-rmse:3.68819\n",
      "[-0.67639124  0.03510863 -0.22428295 ...  0.14659475 -0.69591987\n",
      " -0.06898895]\n",
      "Fold 1 started at Tue Nov  8 03:41:30 2022\n",
      "[03:41:32] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[03:41:32] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:3.92702\tvalid_data-rmse:3.88268\n",
      "[100]\ttrain-rmse:3.11297\tvalid_data-rmse:3.62006\n",
      "[200]\ttrain-rmse:2.92182\tvalid_data-rmse:3.62506\n",
      "[269]\ttrain-rmse:2.82078\tvalid_data-rmse:3.62787\n",
      "[-0.93589476  0.03663608 -0.38669941 ...  0.29481247 -1.05024704\n",
      " -0.06887903]\n",
      "Fold 2 started at Tue Nov  8 04:02:37 2022\n",
      "[04:02:39] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:02:39] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:3.91709\tvalid_data-rmse:3.92482\n",
      "[100]\ttrain-rmse:3.10929\tvalid_data-rmse:3.63884\n",
      "[200]\ttrain-rmse:2.93844\tvalid_data-rmse:3.63712\n",
      "[300]\ttrain-rmse:2.78812\tvalid_data-rmse:3.64050\n",
      "[348]\ttrain-rmse:2.72371\tvalid_data-rmse:3.64298\n",
      "[-1.23585877  0.00986036 -0.56976692 ...  0.47980195 -1.46885952\n",
      " -0.04637819]\n",
      "Fold 3 started at Tue Nov  8 04:29:33 2022\n",
      "[04:29:35] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:29:35] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:3.91961\tvalid_data-rmse:3.91489\n",
      "[100]\ttrain-rmse:3.14646\tvalid_data-rmse:3.61902\n",
      "[200]\ttrain-rmse:2.96853\tvalid_data-rmse:3.62537\n",
      "[297]\ttrain-rmse:2.82294\tvalid_data-rmse:3.63045\n",
      "[-1.49773252 -0.02130515 -0.76194775 ...  0.65026009 -2.11718693\n",
      "  0.00736972]\n",
      "Fold 4 started at Tue Nov  8 04:52:55 2022\n",
      "[04:52:57] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:52:57] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:3.90318\tvalid_data-rmse:3.97732\n",
      "[100]\ttrain-rmse:3.12389\tvalid_data-rmse:3.70680\n",
      "[200]\ttrain-rmse:2.94505\tvalid_data-rmse:3.71093\n",
      "[271]\ttrain-rmse:2.83422\tvalid_data-rmse:3.71498\n",
      "[-2.01574296 -0.06657154 -0.91012397 ...  0.79891126 -2.47974107\n",
      "  0.03561005]\n",
      "CV mean score: 3.6500, std: 0.0337.\n",
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Tue Nov  8 05:14:02 2022\n",
      "[05:14:04] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[05:14:04] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:1.77376\tvalid_data-rmse:1.77804\n",
      "[100]\ttrain-rmse:1.34988\tvalid_data-rmse:1.55728\n",
      "[200]\ttrain-rmse:1.28724\tvalid_data-rmse:1.55818\n",
      "[287]\ttrain-rmse:1.23231\tvalid_data-rmse:1.55995\n",
      "[-0.02759302 -0.02474646 -0.07161566 ...  0.17954008 -0.11230347\n",
      "  0.04098161]\n",
      "Fold 1 started at Tue Nov  8 05:36:46 2022\n",
      "[05:36:48] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[05:36:48] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:1.77414\tvalid_data-rmse:1.77657\n",
      "[100]\ttrain-rmse:1.35482\tvalid_data-rmse:1.55200\n",
      "[200]\ttrain-rmse:1.29058\tvalid_data-rmse:1.55283\n",
      "[300]\ttrain-rmse:1.22561\tvalid_data-rmse:1.55476\n",
      "[307]\ttrain-rmse:1.22177\tvalid_data-rmse:1.55484\n",
      "[-0.05209724 -0.04016835 -0.12880031 ...  0.28999549 -0.2569662\n",
      "  0.10608242]\n",
      "Fold 2 started at Tue Nov  8 06:01:04 2022\n",
      "[06:01:06] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[06:01:06] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:1.77631\tvalid_data-rmse:1.76736\n",
      "[100]\ttrain-rmse:1.35861\tvalid_data-rmse:1.54745\n",
      "[200]\ttrain-rmse:1.29147\tvalid_data-rmse:1.54926\n",
      "[297]\ttrain-rmse:1.23375\tvalid_data-rmse:1.55094\n",
      "[-0.11652234 -0.07436763 -0.18518793 ...  0.39563452 -0.38299701\n",
      "  0.11993176]\n",
      "Fold 3 started at Tue Nov  8 06:24:48 2022\n",
      "[06:24:51] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[06:24:51] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:1.76997\tvalid_data-rmse:1.79372\n",
      "[100]\ttrain-rmse:1.35354\tvalid_data-rmse:1.56270\n",
      "[200]\ttrain-rmse:1.28787\tvalid_data-rmse:1.56297\n",
      "[300]\ttrain-rmse:1.22782\tvalid_data-rmse:1.56408\n",
      "[335]\ttrain-rmse:1.20771\tvalid_data-rmse:1.56484\n",
      "[-0.14226691 -0.06155504 -0.2439201  ...  0.5913544  -0.57211854\n",
      "  0.16924038]\n",
      "Fold 4 started at Tue Nov  8 06:51:29 2022\n",
      "[06:51:31] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[06:51:31] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:1.77548\tvalid_data-rmse:1.77186\n",
      "[100]\ttrain-rmse:1.35410\tvalid_data-rmse:1.55004\n",
      "[200]\ttrain-rmse:1.28928\tvalid_data-rmse:1.55076\n",
      "[298]\ttrain-rmse:1.22857\tvalid_data-rmse:1.55258\n",
      "[-0.21321425 -0.08324108 -0.33494639 ...  0.8136806  -0.74559728\n",
      "  0.19791948]\n",
      "CV mean score: 1.5536, std: 0.0052.\n",
      "========== 分类模型 ==========\n",
      "Fold 0 started at Tue Nov  8 07:15:24 2022\n",
      "[07:16:42] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"metric\", \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:0.47607\tvalid_data-rmse:0.47608\n",
      "[100]\ttrain-rmse:0.09519\tvalid_data-rmse:0.10018\n",
      "[200]\ttrain-rmse:0.09065\tvalid_data-rmse:0.10021\n",
      "[300]\ttrain-rmse:0.08603\tvalid_data-rmse:0.10026\n",
      "[324]\ttrain-rmse:0.08490\tvalid_data-rmse:0.10032\n",
      "[0.01087769 0.00078119 0.0018171  ... 0.00177418 0.01571705 0.00120292]\n",
      "Fold 1 started at Tue Nov  8 07:37:01 2022\n",
      "[07:38:44] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"metric\", \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:0.47607\tvalid_data-rmse:0.47606\n",
      "[100]\ttrain-rmse:0.09574\tvalid_data-rmse:0.09779\n",
      "[200]\ttrain-rmse:0.09122\tvalid_data-rmse:0.09789\n",
      "[300]\ttrain-rmse:0.08658\tvalid_data-rmse:0.09811\n",
      "[317]\ttrain-rmse:0.08576\tvalid_data-rmse:0.09809\n",
      "[0.02189607 0.00148674 0.00376262 ... 0.00333631 0.0404066  0.00254099]\n",
      "Fold 2 started at Tue Nov  8 07:57:49 2022\n",
      "[07:57:52] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"metric\", \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:0.47608\tvalid_data-rmse:0.47608\n",
      "[100]\ttrain-rmse:0.09557\tvalid_data-rmse:0.09904\n",
      "[200]\ttrain-rmse:0.09106\tvalid_data-rmse:0.09892\n",
      "[300]\ttrain-rmse:0.08630\tvalid_data-rmse:0.09896\n",
      "[372]\ttrain-rmse:0.08301\tvalid_data-rmse:0.09907\n",
      "[0.02847655 0.00194646 0.00683968 ... 0.00493586 0.05402095 0.004056  ]\n",
      "Fold 3 started at Tue Nov  8 08:21:05 2022\n",
      "[08:21:08] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"metric\", \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:0.47608\tvalid_data-rmse:0.47607\n",
      "[100]\ttrain-rmse:0.09579\tvalid_data-rmse:0.09849\n",
      "[200]\ttrain-rmse:0.09145\tvalid_data-rmse:0.09856\n",
      "[300]\ttrain-rmse:0.08688\tvalid_data-rmse:0.09871\n",
      "[332]\ttrain-rmse:0.08537\tvalid_data-rmse:0.09883\n",
      "[0.03695822 0.00251134 0.0091187  ... 0.00702273 0.06684331 0.00507859]\n",
      "Fold 4 started at Tue Nov  8 08:42:01 2022\n",
      "[08:42:04] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"metric\", \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:0.47606\tvalid_data-rmse:0.47610\n",
      "[100]\ttrain-rmse:0.09516\tvalid_data-rmse:0.10094\n",
      "[200]\ttrain-rmse:0.09075\tvalid_data-rmse:0.10091\n",
      "[300]\ttrain-rmse:0.08603\tvalid_data-rmse:0.10119\n",
      "[363]\ttrain-rmse:0.08308\tvalid_data-rmse:0.10130\n",
      "[0.04376755 0.00310026 0.01094828 ... 0.00935287 0.08372459 0.00633353]\n",
      "CV mean score: 0.0441, std: 0.0009.\n"
     ]
    }
   ],
   "source": [
    "#### xgb\n",
    "xgb_params = {'eta':0.05, 'max_leaves':47, 'max_depth':10, 'subsample':0.8, 'colsample_bytree':0.8,\n",
    "              'min_child_weight':40, 'max_bin':128, 'reg_alpha':2.0, 'reg_lambda':2.0, \n",
    "              'objective':'reg:linear', 'eval_metric':'rmse', 'silent': True, 'nthread':4}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_xgb , predictions_xgb , scores_xgb  = train_model(X_train , X_test, y_train , params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nxgb, predictions_nxgb, scores_nxgb = train_model(X_ntrain, X_test, y_ntrain, params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "xgb_params['objective'] = 'binary:logistic'\n",
    "xgb_params['metric']    = 'binary_logloss'\n",
    "oof_bxgb, predictions_bxgb, scores_bxgb = train_model(X_train , X_test, y_train_binary, params=xgb_params, folds=folds, model_type='xgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后进行结果保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_xgb\n",
    "sub_df.to_csv('predictions_xgb.csv', index=False)\n",
    "\n",
    "oof_xgb  = pd.DataFrame(oof_xgb)\n",
    "oof_nxgb = pd.DataFrame(oof_nxgb)\n",
    "oof_bxgb = pd.DataFrame(oof_bxgb)\n",
    "\n",
    "predictions_xgb  = pd.DataFrame(predictions_xgb)\n",
    "predictions_nxgb = pd.DataFrame(predictions_nxgb)\n",
    "predictions_bxgb = pd.DataFrame(predictions_bxgb)\n",
    "\n",
    "oof_xgb.to_csv('./result/oof_xgb.csv',header=None,index=False)\n",
    "oof_bxgb.to_csv('./result/oof_bxgb.csv',header=None,index=False)\n",
    "oof_nxgb.to_csv('./result/oof_nxgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_xgb.to_csv('./result/predictions_xgb.csv',header=None,index=False)\n",
    "predictions_nxgb.to_csv('./result/predictions_nxgb.csv',header=None,index=False)\n",
    "predictions_bxgb.to_csv('./result/predictions_bxgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来提交单模型预测结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/YC6S7MjxDWKat3v.png\" alt=\"image-20211210190126611\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进行CatBoost模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Tue Nov  8 09:04:30 2022\n",
      "0:\tlearn: 3.8382284\ttest: 3.8316439\tbest: 3.8316439 (0)\ttotal: 252ms\tremaining: 1h 23m 58s\n",
      "100:\tlearn: 3.5856499\ttest: 3.6420905\tbest: 3.6420905 (100)\ttotal: 12s\tremaining: 39m 15s\n",
      "200:\tlearn: 3.5344058\ttest: 3.6329001\tbest: 3.6329001 (200)\ttotal: 23.1s\tremaining: 37m 52s\n",
      "300:\tlearn: 3.5056632\ttest: 3.6309079\tbest: 3.6308708 (298)\ttotal: 33.3s\tremaining: 36m 22s\n",
      "400:\tlearn: 3.4752138\ttest: 3.6285546\tbest: 3.6285546 (400)\ttotal: 44.2s\tremaining: 36m 2s\n",
      "500:\tlearn: 3.4423441\ttest: 3.6273746\tbest: 3.6273097 (495)\ttotal: 55.3s\tremaining: 35m 54s\n",
      "600:\tlearn: 3.4078193\ttest: 3.6258241\tbest: 3.6258241 (600)\ttotal: 1m 6s\tremaining: 35m 35s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.625496459\n",
      "bestIteration = 639\n",
      "\n",
      "Shrink model to first 640 iterations.\n",
      "[-0.77442644 -0.05924466 -0.18858233 ...  0.14830742 -0.4705812\n",
      "  0.0130593 ]\n",
      "Fold 1 started at Tue Nov  8 09:06:54 2022\n",
      "0:\tlearn: 3.8373500\ttest: 3.8422497\tbest: 3.8422497 (0)\ttotal: 190ms\tremaining: 1h 3m 21s\n",
      "100:\tlearn: 3.5669203\ttest: 3.6682959\tbest: 3.6682959 (100)\ttotal: 10.7s\tremaining: 35m 9s\n",
      "200:\tlearn: 3.5222651\ttest: 3.6617434\tbest: 3.6617391 (198)\ttotal: 21.1s\tremaining: 34m 40s\n",
      "300:\tlearn: 3.4888627\ttest: 3.6592746\tbest: 3.6592457 (298)\ttotal: 31.8s\tremaining: 34m 39s\n",
      "400:\tlearn: 3.4625459\ttest: 3.6576160\tbest: 3.6576093 (398)\ttotal: 41.7s\tremaining: 34m\n",
      "500:\tlearn: 3.4296615\ttest: 3.6564905\tbest: 3.6562826 (486)\ttotal: 51.5s\tremaining: 33m 22s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.656282563\n",
      "bestIteration = 486\n",
      "\n",
      "Shrink model to first 487 iterations.\n",
      "[-1.40786082 -0.13415273 -0.30112423 ...  0.31440546 -1.10832156\n",
      "  0.06448059]\n",
      "Fold 2 started at Tue Nov  8 09:09:07 2022\n",
      "0:\tlearn: 3.8361905\ttest: 3.8392469\tbest: 3.8392469 (0)\ttotal: 156ms\tremaining: 51m 56s\n",
      "100:\tlearn: 3.5747120\ttest: 3.6702959\tbest: 3.6702898 (99)\ttotal: 9.51s\tremaining: 31m 13s\n",
      "200:\tlearn: 3.5287291\ttest: 3.6659733\tbest: 3.6657924 (197)\ttotal: 18.6s\tremaining: 30m 30s\n",
      "300:\tlearn: 3.4997837\ttest: 3.6636791\tbest: 3.6636514 (294)\ttotal: 27.9s\tremaining: 30m 25s\n",
      "400:\tlearn: 3.4723211\ttest: 3.6626714\tbest: 3.6625277 (362)\ttotal: 37.3s\tremaining: 30m 21s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.662088879\n",
      "bestIteration = 425\n",
      "\n",
      "Shrink model to first 426 iterations.\n",
      "[-2.11760547 -0.20081861 -0.47951447 ...  0.46288107 -1.68176641\n",
      "  0.13453699]\n",
      "Fold 3 started at Tue Nov  8 09:11:02 2022\n",
      "0:\tlearn: 3.8414221\ttest: 3.8238792\tbest: 3.8238792 (0)\ttotal: 159ms\tremaining: 52m 53s\n",
      "100:\tlearn: 3.5777596\ttest: 3.6388521\tbest: 3.6388521 (100)\ttotal: 10.7s\tremaining: 35m 15s\n",
      "200:\tlearn: 3.5224667\ttest: 3.6309975\tbest: 3.6309975 (200)\ttotal: 21.5s\tremaining: 35m 21s\n",
      "300:\tlearn: 3.4839285\ttest: 3.6286389\tbest: 3.6283923 (286)\ttotal: 32.5s\tremaining: 35m 24s\n",
      "400:\tlearn: 3.4462644\ttest: 3.6265033\tbest: 3.6265033 (400)\ttotal: 42.6s\tremaining: 34m 43s\n",
      "500:\tlearn: 3.4126937\ttest: 3.6251088\tbest: 3.6251088 (500)\ttotal: 52.5s\tremaining: 34m 1s\n",
      "600:\tlearn: 3.3806990\ttest: 3.6242900\tbest: 3.6240502 (594)\ttotal: 1m 2s\tremaining: 33m 33s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.624050203\n",
      "bestIteration = 594\n",
      "\n",
      "Shrink model to first 595 iterations.\n",
      "[-2.63416371 -0.30107807 -0.675056   ...  0.62920866 -2.32790749\n",
      "  0.16057907]\n",
      "Fold 4 started at Tue Nov  8 09:13:06 2022\n",
      "0:\tlearn: 3.8324085\ttest: 3.8564726\tbest: 3.8564726 (0)\ttotal: 167ms\tremaining: 55m 46s\n",
      "100:\tlearn: 3.5710972\ttest: 3.6865793\tbest: 3.6865793 (100)\ttotal: 10.5s\tremaining: 34m 35s\n",
      "200:\tlearn: 3.5276363\ttest: 3.6805183\tbest: 3.6805183 (200)\ttotal: 20.8s\tremaining: 34m 10s\n",
      "300:\tlearn: 3.4959093\ttest: 3.6784156\tbest: 3.6783323 (294)\ttotal: 31.6s\tremaining: 34m 27s\n",
      "400:\tlearn: 3.4626956\ttest: 3.6773356\tbest: 3.6771748 (379)\ttotal: 41.8s\tremaining: 34m 4s\n",
      "500:\tlearn: 3.4276919\ttest: 3.6755139\tbest: 3.6754203 (486)\ttotal: 52.3s\tremaining: 33m 54s\n",
      "600:\tlearn: 3.3939305\ttest: 3.6752012\tbest: 3.6750674 (553)\ttotal: 1m 2s\tremaining: 33m 21s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.675067442\n",
      "bestIteration = 553\n",
      "\n",
      "Shrink model to first 554 iterations.\n",
      "[-3.3263679  -0.39961787 -0.89724422 ...  0.82225482 -2.95446929\n",
      "  0.1827579 ]\n",
      "CV mean score: 3.6486, std: 0.0204.\n",
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Tue Nov  8 09:15:25 2022\n",
      "0:\tlearn: 1.7045330\ttest: 1.7141846\tbest: 1.7141846 (0)\ttotal: 178ms\tremaining: 59m 10s\n",
      "100:\tlearn: 1.5415638\ttest: 1.5672116\tbest: 1.5672116 (100)\ttotal: 11.3s\tremaining: 37m 1s\n",
      "200:\tlearn: 1.5202918\ttest: 1.5611541\tbest: 1.5611541 (200)\ttotal: 22.1s\tremaining: 36m 16s\n",
      "300:\tlearn: 1.5040674\ttest: 1.5589317\tbest: 1.5589317 (300)\ttotal: 32.6s\tremaining: 35m 34s\n",
      "400:\tlearn: 1.4874907\ttest: 1.5581905\tbest: 1.5581634 (381)\ttotal: 43.6s\tremaining: 35m 30s\n",
      "500:\tlearn: 1.4736180\ttest: 1.5575935\tbest: 1.5575935 (500)\ttotal: 54.5s\tremaining: 35m 22s\n",
      "600:\tlearn: 1.4586537\ttest: 1.5573693\tbest: 1.5573522 (599)\ttotal: 1m 5s\tremaining: 35m 9s\n",
      "700:\tlearn: 1.4445663\ttest: 1.5575412\tbest: 1.5572951 (657)\ttotal: 1m 15s\tremaining: 34m 44s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.557295088\n",
      "bestIteration = 657\n",
      "\n",
      "Shrink model to first 658 iterations.\n",
      "[-0.09131633 -0.05866992 -0.0989837  ...  0.17215502 -0.16379993\n",
      "  0.02516456]\n",
      "Fold 1 started at Tue Nov  8 09:17:52 2022\n",
      "0:\tlearn: 1.7107917\ttest: 1.6936264\tbest: 1.6936264 (0)\ttotal: 178ms\tremaining: 59m 28s\n",
      "100:\tlearn: 1.5455295\ttest: 1.5495673\tbest: 1.5495673 (100)\ttotal: 11.9s\tremaining: 38m 55s\n",
      "200:\tlearn: 1.5247539\ttest: 1.5442838\tbest: 1.5442670 (199)\ttotal: 22.6s\tremaining: 37m 5s\n",
      "300:\tlearn: 1.5072100\ttest: 1.5423675\tbest: 1.5423675 (300)\ttotal: 31.9s\tremaining: 34m 47s\n",
      "400:\tlearn: 1.4912705\ttest: 1.5414349\tbest: 1.5413887 (397)\ttotal: 41.1s\tremaining: 33m 29s\n",
      "500:\tlearn: 1.4770456\ttest: 1.5409977\tbest: 1.5409890 (499)\ttotal: 50.4s\tremaining: 32m 42s\n",
      "600:\tlearn: 1.4616586\ttest: 1.5407039\tbest: 1.5406684 (575)\ttotal: 59.7s\tremaining: 32m 7s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.5405737\n",
      "bestIteration = 620\n",
      "\n",
      "Shrink model to first 621 iterations.\n",
      "[-0.16660542 -0.10774404 -0.20112206 ...  0.38406568 -0.33222521\n",
      "  0.05941586]\n",
      "Fold 2 started at Tue Nov  8 09:20:02 2022\n",
      "0:\tlearn: 1.7119234\ttest: 1.6870972\tbest: 1.6870972 (0)\ttotal: 141ms\tremaining: 46m 52s\n",
      "100:\tlearn: 1.5466862\ttest: 1.5435938\tbest: 1.5435938 (100)\ttotal: 9.75s\tremaining: 32m\n",
      "200:\tlearn: 1.5255889\ttest: 1.5382787\tbest: 1.5382656 (199)\ttotal: 19.3s\tremaining: 31m 38s\n",
      "300:\tlearn: 1.5091344\ttest: 1.5362775\tbest: 1.5362775 (300)\ttotal: 28.7s\tremaining: 31m 15s\n",
      "400:\tlearn: 1.4938708\ttest: 1.5355266\tbest: 1.5355167 (399)\ttotal: 38s\tremaining: 30m 58s\n",
      "500:\tlearn: 1.4800472\ttest: 1.5352190\tbest: 1.5352190 (500)\ttotal: 47.3s\tremaining: 30m 42s\n",
      "600:\tlearn: 1.4657367\ttest: 1.5349376\tbest: 1.5349318 (596)\ttotal: 57s\tremaining: 30m 39s\n",
      "700:\tlearn: 1.4516925\ttest: 1.5349477\tbest: 1.5349186 (650)\ttotal: 1m 7s\tremaining: 30m 53s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.534918561\n",
      "bestIteration = 650\n",
      "\n",
      "Shrink model to first 651 iterations.\n",
      "[-0.22754232 -0.17767282 -0.30042562 ...  0.56371498 -0.498243\n",
      "  0.09114865]\n",
      "Fold 3 started at Tue Nov  8 09:22:24 2022\n",
      "0:\tlearn: 1.7012196\ttest: 1.7292957\tbest: 1.7292957 (0)\ttotal: 200ms\tremaining: 1h 6m 32s\n",
      "100:\tlearn: 1.5404993\ttest: 1.5716385\tbest: 1.5716385 (100)\ttotal: 11.1s\tremaining: 36m 25s\n",
      "200:\tlearn: 1.5188510\ttest: 1.5648273\tbest: 1.5648273 (200)\ttotal: 22.1s\tremaining: 36m 14s\n",
      "300:\tlearn: 1.5018983\ttest: 1.5625756\tbest: 1.5625756 (300)\ttotal: 32.9s\tremaining: 35m 50s\n",
      "400:\tlearn: 1.4854678\ttest: 1.5611901\tbest: 1.5611901 (400)\ttotal: 43.6s\tremaining: 35m 30s\n",
      "500:\tlearn: 1.4707036\ttest: 1.5604726\tbest: 1.5604669 (499)\ttotal: 54.3s\tremaining: 35m 12s\n",
      "600:\tlearn: 1.4571320\ttest: 1.5598978\tbest: 1.5598978 (600)\ttotal: 1m 5s\tremaining: 34m 58s\n",
      "700:\tlearn: 1.4436513\ttest: 1.5593972\tbest: 1.5593972 (700)\ttotal: 1m 15s\tremaining: 34m 42s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.559272271\n",
      "bestIteration = 740\n",
      "\n",
      "Shrink model to first 741 iterations.\n",
      "[-0.27653528 -0.17639465 -0.37722065 ...  0.76679197 -0.64621324\n",
      "  0.14261736]\n",
      "Fold 4 started at Tue Nov  8 09:24:51 2022\n",
      "0:\tlearn: 1.7059081\ttest: 1.7106650\tbest: 1.7106650 (0)\ttotal: 153ms\tremaining: 51m 2s\n",
      "100:\tlearn: 1.5409680\ttest: 1.5673913\tbest: 1.5673913 (100)\ttotal: 11.3s\tremaining: 37m 7s\n",
      "200:\tlearn: 1.5200042\ttest: 1.5620749\tbest: 1.5620749 (200)\ttotal: 22.3s\tremaining: 36m 37s\n",
      "300:\tlearn: 1.5031683\ttest: 1.5604798\tbest: 1.5604789 (299)\ttotal: 33.1s\tremaining: 36m 4s\n",
      "400:\tlearn: 1.4883228\ttest: 1.5597109\tbest: 1.5596867 (393)\ttotal: 43.4s\tremaining: 35m 23s\n",
      "500:\tlearn: 1.4741288\ttest: 1.5592373\tbest: 1.5592315 (499)\ttotal: 54s\tremaining: 35m 3s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.559205215\n",
      "bestIteration = 510\n",
      "\n",
      "Shrink model to first 511 iterations.\n",
      "[-0.35979922 -0.23879994 -0.46512951 ...  0.96029791 -0.80157997\n",
      "  0.18719912]\n",
      "CV mean score: 1.5503, std: 0.0104.\n",
      "========== 分类模型 ==========\n",
      "Fold 0 started at Tue Nov  8 09:26:49 2022\n",
      "0:\tlearn: 0.5879833\ttest: 0.5880386\tbest: 0.5880386 (0)\ttotal: 148ms\tremaining: 49m 9s\n",
      "100:\tlearn: 0.0413518\ttest: 0.0441144\tbest: 0.0441144 (100)\ttotal: 12.1s\tremaining: 39m 44s\n",
      "200:\tlearn: 0.0385442\ttest: 0.0436458\tbest: 0.0436458 (200)\ttotal: 23.6s\tremaining: 38m 47s\n",
      "300:\tlearn: 0.0370328\ttest: 0.0435699\tbest: 0.0435643 (292)\ttotal: 34.8s\tremaining: 37m 59s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04353368095\n",
      "bestIteration = 334\n",
      "\n",
      "Shrink model to first 335 iterations.\n",
      "[0.01778881 0.00054769 0.00164088 ... 0.00117843 0.01433617 0.00103438]\n",
      "Fold 1 started at Tue Nov  8 09:28:31 2022\n",
      "0:\tlearn: 0.5877719\ttest: 0.5879553\tbest: 0.5879553 (0)\ttotal: 141ms\tremaining: 46m 55s\n",
      "100:\tlearn: 0.0411742\ttest: 0.0455801\tbest: 0.0455801 (100)\ttotal: 12.2s\tremaining: 39m 55s\n",
      "200:\tlearn: 0.0385596\ttest: 0.0451429\tbest: 0.0451426 (199)\ttotal: 23.6s\tremaining: 38m 44s\n",
      "300:\tlearn: 0.0371186\ttest: 0.0450241\tbest: 0.0450238 (298)\ttotal: 34.8s\tremaining: 37m 56s\n",
      "400:\tlearn: 0.0357976\ttest: 0.0449689\tbest: 0.0449678 (399)\ttotal: 46.3s\tremaining: 37m 41s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04496143836\n",
      "bestIteration = 408\n",
      "\n",
      "Shrink model to first 409 iterations.\n",
      "[0.03669494 0.00109566 0.00280062 ... 0.00213998 0.02672421 0.00226992]\n",
      "Fold 2 started at Tue Nov  8 09:30:22 2022\n",
      "0:\tlearn: 0.5867322\ttest: 0.5868468\tbest: 0.5868468 (0)\ttotal: 138ms\tremaining: 45m 58s\n",
      "100:\tlearn: 0.0416737\ttest: 0.0450366\tbest: 0.0450294 (98)\ttotal: 12.2s\tremaining: 39m 58s\n",
      "200:\tlearn: 0.0389469\ttest: 0.0443730\tbest: 0.0443730 (200)\ttotal: 23.6s\tremaining: 38m 43s\n",
      "300:\tlearn: 0.0371671\ttest: 0.0442113\tbest: 0.0442113 (300)\ttotal: 35s\tremaining: 38m 11s\n",
      "400:\tlearn: 0.0356436\ttest: 0.0441872\tbest: 0.0441858 (392)\ttotal: 46.4s\tremaining: 37m 46s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04416814052\n",
      "bestIteration = 418\n",
      "\n",
      "Shrink model to first 419 iterations.\n",
      "[0.05503395 0.00139273 0.00387239 ... 0.00313825 0.04047133 0.00333988]\n",
      "Fold 3 started at Tue Nov  8 09:32:13 2022\n",
      "0:\tlearn: 0.5864983\ttest: 0.5866568\tbest: 0.5866568 (0)\ttotal: 169ms\tremaining: 56m 12s\n",
      "100:\tlearn: 0.0412729\ttest: 0.0437934\tbest: 0.0437934 (100)\ttotal: 12.1s\tremaining: 39m 47s\n",
      "200:\tlearn: 0.0388131\ttest: 0.0433856\tbest: 0.0433856 (200)\ttotal: 23.7s\tremaining: 38m 50s\n",
      "300:\tlearn: 0.0374291\ttest: 0.0432850\tbest: 0.0432754 (271)\ttotal: 34.9s\tremaining: 38m 6s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04325779579\n",
      "bestIteration = 310\n",
      "\n",
      "Shrink model to first 311 iterations.\n",
      "[0.06703968 0.00160726 0.00553086 ... 0.00424435 0.05286002 0.00467876]\n",
      "Fold 4 started at Tue Nov  8 09:33:53 2022\n",
      "0:\tlearn: 0.5836974\ttest: 0.5837488\tbest: 0.5837488 (0)\ttotal: 145ms\tremaining: 48m 25s\n",
      "100:\tlearn: 0.0413642\ttest: 0.0456157\tbest: 0.0456157 (100)\ttotal: 11.8s\tremaining: 38m 46s\n",
      "200:\tlearn: 0.0387611\ttest: 0.0448779\tbest: 0.0448760 (196)\ttotal: 23.3s\tremaining: 38m 17s\n",
      "300:\tlearn: 0.0371180\ttest: 0.0447422\tbest: 0.0447379 (292)\ttotal: 34.8s\tremaining: 37m 54s\n",
      "400:\tlearn: 0.0356654\ttest: 0.0446772\tbest: 0.0446706 (392)\ttotal: 45.9s\tremaining: 37m 25s\n",
      "500:\tlearn: 0.0341869\ttest: 0.0446481\tbest: 0.0446380 (490)\ttotal: 57.2s\tremaining: 37m 4s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.04463795973\n",
      "bestIteration = 490\n",
      "\n",
      "Shrink model to first 491 iterations.\n",
      "[0.08493056 0.00232892 0.00646678 ... 0.00558998 0.06521076 0.00588113]\n",
      "CV mean score: 0.0441, std: 0.0006.\n"
     ]
    }
   ],
   "source": [
    "#### cat\n",
    "cat_params = {'learning_rate': 0.05, 'depth': 9, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "              'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=18)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_cat , predictions_cat , scores_cat  = train_model(X_train , X_test, y_train , params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_ncat, predictions_ncat, scores_ncat = train_model(X_ntrain, X_test, y_ntrain, params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "oof_bcat, predictions_bcat, scores_bcat = train_model(X_train , X_test, y_train_binary, params=cat_params, folds=folds, model_type='cat', eval_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时保存模型结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_cat\n",
    "sub_df.to_csv('predictions_cat.csv', index=False)\n",
    "\n",
    "oof_cat  = pd.DataFrame(oof_cat)\n",
    "oof_ncat = pd.DataFrame(oof_ncat)\n",
    "oof_bcat = pd.DataFrame(oof_bcat)\n",
    "\n",
    "predictions_cat  = pd.DataFrame(predictions_cat)\n",
    "predictions_ncat = pd.DataFrame(predictions_ncat)\n",
    "predictions_bcat = pd.DataFrame(predictions_bcat)\n",
    "\n",
    "oof_cat.to_csv('./result/oof_cat.csv',header=None,index=False)\n",
    "oof_bcat.to_csv('./result/oof_bcat.csv',header=None,index=False)\n",
    "oof_ncat.to_csv('./result/oof_ncat.csv',header=None,index=False)\n",
    "\n",
    "predictions_cat.to_csv('./result/predictions_cat.csv',header=None,index=False)\n",
    "predictions_ncat.to_csv('./result/predictions_ncat.csv',header=None,index=False)\n",
    "predictions_bcat.to_csv('./result/predictions_bcat.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试单模型建模效果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/IfC6ehVKURszlY1.png\" alt=\"image-20211210190346541\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.融合阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_lgb.values.flatten() + predictions_xgb.values.flatten() + predictions_cat.values.flatten()) / 3\n",
    "sub_df.to_csv('predictions_wei_average.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/61oY7cBnlZrJMvk.png\" alt=\"image-20211210190555770\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |\n",
    "| Voting | 3.60640 | 3.68697 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacking融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/08/ALF3cfuSwmB7b8z.png\" alt=\"image-20211208192640281\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y, eval_type='regression'):\n",
    "   \n",
    "    # Part 1.数据准备\n",
    "    # 按行拼接列，拼接验证集所有预测结果\n",
    "    # train_stack就是final model的训练数据\n",
    "    train_stack = np.hstack([oof_1, oof_2, oof_3])\n",
    "    # 按行拼接列，拼接测试集上所有预测结果\n",
    "    # test_stack就是final model的测试数据\n",
    "    test_stack = np.hstack([predictions_1, predictions_2, predictions_3])\n",
    "    # 创建一个和验证集行数相同的全零数组\n",
    "    oof = np.zeros(train_stack.shape[0])\n",
    "    # 创建一个和测试集行数相同的全零数组\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "    \n",
    "    # Part 2.多轮交叉验证\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)\n",
    "    \n",
    "    # fold_为折数，trn_idx为每一折训练集index，val_idx为每一折验证集index\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):\n",
    "        # 打印折数信息\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        # 训练集中划分为训练数据的特征和标签\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        # 训练集中划分为验证数据的特征和标签\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "        # 开始训练时提示\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_+1) + \"-\" * 10)\n",
    "        # 采用贝叶斯回归作为结果融合的模型（final model）\n",
    "        clf = BayesianRidge()\n",
    "        # 在训练数据上进行训练\n",
    "        clf.fit(trn_data, trn_y)\n",
    "        # 在验证数据上进行预测，并将结果记录在oof对应位置\n",
    "        # oof[val_idx] = clf.predict(val_data)\n",
    "        # 对测试集数据进行预测，每一轮预测结果占比额外的1/10\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "        \n",
    "    if eval_type == 'regression':\n",
    "        print('mean: ',np.sqrt(mean_squared_error(y, oof)))\n",
    "    if eval_type == 'binary':\n",
    "        print('mean: ',log_loss(y, oof))\n",
    "    \n",
    "    # 返回测试集的预测结果\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 1----------\n",
      "fold n°2\n",
      "----------Stacking 2----------\n",
      "fold n°3\n",
      "----------Stacking 3----------\n",
      "fold n°4\n",
      "----------Stacking 4----------\n",
      "fold n°5\n",
      "----------Stacking 5----------\n",
      "fold n°6\n",
      "----------Stacking 6----------\n",
      "fold n°7\n",
      "----------Stacking 7----------\n",
      "fold n°8\n",
      "----------Stacking 8----------\n",
      "fold n°9\n",
      "----------Stacking 9----------\n",
      "fold n°10\n",
      "----------Stacking 10----------\n",
      "mean:  3.8705589161316296\n",
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 1----------\n",
      "fold n°2\n",
      "----------Stacking 2----------\n",
      "fold n°3\n",
      "----------Stacking 3----------\n",
      "fold n°4\n",
      "----------Stacking 4----------\n",
      "fold n°5\n",
      "----------Stacking 5----------\n",
      "fold n°6\n",
      "----------Stacking 6----------\n",
      "fold n°7\n",
      "----------Stacking 7----------\n",
      "fold n°8\n",
      "----------Stacking 8----------\n",
      "fold n°9\n",
      "----------Stacking 9----------\n",
      "fold n°10\n",
      "----------Stacking 10----------\n",
      "mean:  1.718066151175359\n",
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 1----------\n",
      "fold n°2\n",
      "----------Stacking 2----------\n",
      "fold n°3\n",
      "----------Stacking 3----------\n",
      "fold n°4\n",
      "----------Stacking 4----------\n",
      "fold n°5\n",
      "----------Stacking 5----------\n",
      "fold n°6\n",
      "----------Stacking 6----------\n",
      "fold n°7\n",
      "----------Stacking 7----------\n",
      "fold n°8\n",
      "----------Stacking 8----------\n",
      "fold n°9\n",
      "----------Stacking 9----------\n",
      "fold n°10\n",
      "----------Stacking 10----------\n",
      "mean:  0.3775168980500308\n"
     ]
    }
   ],
   "source": [
    "print('='*30)\n",
    "oof_stack , predictions_stack  = stack_model(oof_lgb , oof_xgb , oof_cat , predictions_lgb , predictions_xgb , predictions_cat , target)\n",
    "print('='*30)\n",
    "oof_nstack, predictions_nstack = stack_model(oof_nlgb, oof_nxgb, oof_ncat, predictions_nlgb, predictions_nxgb, predictions_ncat, ntarget)\n",
    "print('='*30)\n",
    "oof_bstack, predictions_bstack = stack_model(oof_blgb, oof_bxgb, oof_bcat, predictions_blgb, predictions_bxgb, predictions_bcat, target_binary, eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv('predictions_stack.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/gIGwtZskjqmYOCh.png\" alt=\"image-20211210190813208\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |\n",
    "| Voting | 3.60640 | 3.68697 |\n",
    "| Stacking | 3.60683 | 3.68217 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trick融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack\n",
    "sub_df.to_csv('predictions_trick.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/BY7C8IaF2zoASst.png\" alt=\"image-20211210190941459\" style=\"zoom: 50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |\n",
    "| Voting | 3.60640 | 3.68697 |\n",
    "| Stacking | 3.60683 | 3.68217 |\n",
    "| Trick | 3.60232 | 3.67452 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TrickStacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack)*0.5 + predictions_stack*0.5\n",
    "sub_df.to_csv('predictions_trick&stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://s2.loli.net/2021/12/10/T3VM6OZ9PlkhxLG.png\" alt=\"image-20211210191045474\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | Private Score | Public Score |\n",
    "| ------ | ------ | ------ |\n",
    "| randomforest | 3.65455 | 3.74969 |\n",
    "| randomforest+validation | 3.65173 | 3.74954 |\n",
    "| LightGBM | 3.69723 | 3.80436 |\n",
    "| LightGBM+validation | 3.64403 | 3.73875 |\n",
    "| XGBoost | 3.62832 | 3.72358 |\n",
    "| Voting_avr | 3.63650 | 3.73251 |\n",
    "| Voting_wei | 3.633307 | 3.72877 |\n",
    "| Stacking | 3.62798 | 3.72055 |\n",
    "| 第二阶段 | 特征优化 | 两阶段建模 |\n",
    "| LightGBM | 3.61177 | 3.68321 |\n",
    "| XGBoost | 3.61048 | 3.68938 |\n",
    "| CatBoost | 3.61154 | 3.69951 |\n",
    "| Voting | 3.60640 | 3.68697 |\n",
    "| Stacking | 3.60683 | 3.68217 |\n",
    "| Trick | 3.60232 | 3.67452 |\n",
    "| Trick+Voting | 3.60206 | 3.67580 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比目前该比赛的榜单，我们能够看到最后的成绩排名，私榜排名如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gx8ylfiay1j31f80dstat.jpg\" alt=\"image-20211023185341867\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006784589290041192"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28 / 4127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "排名约在前0.6%，而公榜排名如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://tva1.sinaimg.cn/large/008i3skNly1gx8yll24v2j31g60doq4g.jpg\" alt=\"image-20211023185341867\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033196026169130116"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "137 / 4127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "约在前3%左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 后续优化策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;无论目前结果如何，永远可以有更进一步优化的地方。相信细心的小伙伴已经观察到，本节特征构建和建模策略和此前5天的策略略有差异，如果能融合两部分的方法，则能取得更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型优化和特征优化\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
